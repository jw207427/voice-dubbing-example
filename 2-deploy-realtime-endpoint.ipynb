{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy SageMaker Endpoint\n",
    "The following section deploys the tortoise-tts model to SageMaker as an async inference endpoint.\n",
    "\n",
    "\n",
    "__Prerequisites__\n",
    "- Models must be in a model.tar.gz containing the fine-tuned autoregressive models and other models for tortoise-tts to run\n",
    "- The execution role must have S3 read write access to the input and output bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel  \n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_s3\n",
    "%store -r prefix\n",
    "%store -r bucket_name\n",
    "\n",
    "endpoint_name = name_from_base(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint tortoise-tts-2024-05-15-16-51-36-218: Failed. Reason: Unable to provision requested ML compute capacity due to InsufficientInstanceCapacity error. Please retry using a different ML instance type or after some time.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PyTorchModel(\n\u001b[1;32m      2\u001b[0m     source_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mml.g5.xlarge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:1662\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1660\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1675\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5635\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5632\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   5633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 5635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5641\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:4493\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[1;32m   4490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_arn \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   4492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4493\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5278\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[1;32m   5272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   5273\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   5274\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5275\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5276\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5277\u001b[0m         )\n\u001b[0;32m-> 5278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   5279\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5280\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5281\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5282\u001b[0m     )\n\u001b[1;32m   5283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint tortoise-tts-2024-05-15-16-51-36-218: Failed. Reason: Unable to provision requested ML compute capacity due to InsufficientInstanceCapacity error. Please retry using a different ML instance type or after some time.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "model = PyTorchModel(\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    model_data=model_s3,\n",
    "    framework_version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    role=get_execution_role(),\n",
    "    env={'SAGEMAKER_TS_RESPONSE_TIMEOUT': '900'}\n",
    "    \n",
    ")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >  Test endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text\": \"The lead engineer, a confident woman, stands before them, her presentation deck loaded.\",\n",
    "    \"voice_samples\": \"adam\",\n",
    "    \"inference_params\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = [\n",
    "    \"In a sleek, modern office setting, a group of A W S employees gather around a conference table, laptops open and energy palpable.\",\n",
    "    \"The lead engineer, a confident woman, stands before them, her presentation deck loaded.\",\n",
    "    \"The Azure team, led by a suave executive, strides into the room, exuding an air of casual arrogance.\",\n",
    "    \"They take their seats, exchanging taunting glances with the AWS team. The tension is thick enough to cut with a knife.\",\n",
    "    \"The A W S engineer launches into her presentation, highlighting the superiority of their cloud services with intricate diagrams and impressive statistics.\", \n",
    "    \"The Azure team members roll their eyes and scribble snarky comments on their notepads.\",\n",
    "    \"The Azure executive stands up, his fingers tapping his tablet in a show of bravado.\",\n",
    "    \"He begins a counterattack, boasting about Azure's scalability and cost-effectiveness. The A W S team leans back, arms crossed, unimpressed.\",\n",
    "    \"The battle rages on, with each side one-upping the other's claims, using increasingly outlandish metaphors and analogies.\", \n",
    "    \"The tension gives way to absurdity as the arguments become more and more exaggerated.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'In a sleek, modern office setting, a group of A W S employees gather around a conference table, laptops open and energy palpable.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-0.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-1.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': 'The lead engineer, a confident woman, stands before them, her presentation deck loaded.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-1.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-2.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': 'The Azure team, led by a suave executive, strides into the room, exuding an air of casual arrogance.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-2.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-3.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': 'They take their seats, exchanging taunting glances with the AWS team. The tension is thick enough to cut with a knife.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-3.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-4.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': 'The A W S engineer launches into her presentation, highlighting the superiority of their cloud services with intricate diagrams and impressive statistics.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-4.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-5.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': 'The Azure team members roll their eyes and scribble snarky comments on their notepads.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-5.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-6.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': 'The Azure executive stands up, his fingers tapping his tablet in a show of bravado.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-6.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-7.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': \"He begins a counterattack, boasting about Azure's scalability and cost-effectiveness. The A W S team leans back, arms crossed, unimpressed.\",\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-7.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-8.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': \"The battle rages on, with each side one-upping the other's claims, using increasingly outlandish metaphors and analogies.\",\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-8.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-9.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}},\n",
       " {'text': 'The tension gives way to absurdity as the arguments become more and more exaggerated.',\n",
       "  'voice_samples_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/voice-samples/adam',\n",
       "  'input_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/inputs/adam-input-part-9.json',\n",
       "  'destination_s3_uri': 's3://sagemaker-us-east-1-372703588567/tortoise-tts/outputs/adam-output-10.wav',\n",
       "  'model_id': '',\n",
       "  'inference_params': {}}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads = []\n",
    "\n",
    "for i, s in enumerate(story):\n",
    "    payload = request_template.copy()\n",
    "    payload[\"text\"] =s\n",
    "    input_file = f\"s3://{bucket_name}/{prefix}/inputs/adam-input-part-{i}.json\"\n",
    "    output_file = f\"s3://{bucket_name}/{prefix}/outputs/adam-output-{i+1}.wav\"\n",
    "\n",
    "    payload[\"voice_samples_s3_uri\"] =sample_s3\n",
    "    \n",
    "    payload[\"input_s3_uri\"]=input_file\n",
    "    payload[\"destination_s3_uri\"]=output_file\n",
    "        \n",
    "    payloads.append(payload)\n",
    "\n",
    "payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
